{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycaret\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\Users\\Hamdi\\Projects\\GNScraper\\data')\n",
    "\n",
    "# Load Dataset\n",
    "true_data = pd.read_csv('a1_True.csv')\n",
    "fake_data = pd.read_csv('a2_Fake.csv')\n",
    "\n",
    "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
    "true_data['Target'] = ['True']*len(true_data)\n",
    "fake_data['Target'] = ['Fake']*len(fake_data)\n",
    "\n",
    "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
    "data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n",
    "\n",
    "# See how the data looks like\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = pd.get_dummies(data.Target)['Fake']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if our data is well balanced\n",
    "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
    "plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test-split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test set split into 70:15:15 ratio\n",
    "# Train-Temp split\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'],\n",
    "                                                                    random_state=2018,\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    stratify=data['Target'])\n",
    "# Validation-Test split\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                                                                random_state=2018,\n",
    "                                                                test_size=0.5,\n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Fine-tuning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pretrained BERT Model\n",
    "# Load BERT model and tokenizer via HuggingFace Transformers\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare Input Data\n",
    "# Plot histogram of the number of words in train data 'title'\n",
    "seq_len = [len(title.split()) for title in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of texts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of titles above have word length under 15. So, we set max title length as 15\n",
    "MAX_LENGHT = 15\n",
    "# Tokenize and encode sequences in the train set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader structure definition\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32                                               #define a batch size\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)    # wrap tensors\n",
    "train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "                                                              # dataLoader for train set\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)            # wrap tensors\n",
    "val_sampler = SequentialSampler(val_data)                     # sampler for sampling the data during training\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "                                                              # dataLoader for validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze Layers\n",
    "\n",
    "# Freezing the parameters and defining trainable BERT structure\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False    # false here means gradient need not be computed\n",
    "\n",
    "## Define Model Architecture\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "      super(BERT_Arch, self).__init__()\n",
    "      self.bert = bert\n",
    "      self.dropout = nn.Dropout(0.1)            # dropout layer\n",
    "      self.relu =  nn.ReLU()                    # relu activation function\n",
    "      self.fc1 = nn.Linear(768,512)             # dense layer 1\n",
    "      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function\n",
    "    def forward(self, sent_id, mask):           # define the forward pass\n",
    "      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "                                                # pass the inputs to the model\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)                           # output layer\n",
    "      x = self.softmax(x)                       # apply softmax activation\n",
    "      return x\n",
    "\n",
    "model = BERT_Arch(bert)\n",
    "# Defining the hyperparameters (optimizer, weights of the classes and the epochs)\n",
    "# Define the optimizer\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate\n",
    "# Define the loss function\n",
    "cross_entropy  = nn.NLLLoss()\n",
    "# Number of training epochs\n",
    "epochs = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Train & Evaluate Function\n",
    "\n",
    "# Defining training and evaluation functions\n",
    "def train():\n",
    "  model.train()\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "\n",
    "  for step,batch in enumerate(train_dataloader):                # iterate over batches\n",
    "    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "    batch = [r for r in batch]                                  # push the batch to gpu\n",
    "    sent_id, mask, labels = batch\n",
    "    model.zero_grad()                                           # clear previously calculated gradients\n",
    "    preds = model(sent_id, mask)                                # get model predictions for current batch\n",
    "    loss = cross_entropy(preds, labels)                         # compute loss between actual & predicted values\n",
    "    total_loss = total_loss + loss.item()                       # add on to the total loss\n",
    "    loss.backward()                                             # backward pass to calculate the gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)     # clip gradients to 1.0. It helps in preventing exploding gradient problem\n",
    "    optimizer.step()                                            # update parameters\n",
    "    preds=preds.detach().cpu().numpy()                          # model predictions are stored on GPU. So, push it to CPU\n",
    "\n",
    "  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch\n",
    "                                                                # reshape predictions in form of (# samples, # classes)\n",
    "  return avg_loss                                 # returns the loss and predictions\n",
    "\n",
    "def evaluate():\n",
    "  print(\"\\nEvaluating...\")\n",
    "  model.eval()                                    # Deactivate dropout layers\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  for step,batch in enumerate(val_dataloader):    # Iterate over batches\n",
    "    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.\n",
    "                                                  # Calculate elapsed time in minutes.\n",
    "                                                  # Elapsed = format_time(time.time() - t0)\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "                                                  # Report progress\n",
    "    batch = [t for t in batch]                    # Push the batch to GPU\n",
    "    sent_id, mask, labels = batch\n",
    "    with torch.no_grad():                         # Deactivate autograd\n",
    "      preds = model(sent_id, mask)                # Model predictions\n",
    "      loss = cross_entropy(preds,labels)          # Compute the validation loss between actual and predicted values\n",
    "      total_loss = total_loss + loss.item()\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "  avg_loss = total_loss / len(val_dataloader)         # compute the validation loss of the epoch\n",
    "  return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and predict\n",
    "best_valid_loss = float('inf')\n",
    "train_losses=[]                   # empty lists to store training and validation loss of each epoch\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    train_loss = train()                       # train model\n",
    "    valid_loss = evaluate()                    # evaluate model\n",
    "    if valid_loss < best_valid_loss:              # save the best model\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'c2_new_model_weights.pt')\n",
    "    train_losses.append(train_loss)               # append training and validation loss\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model performance\n",
    "\n",
    "# load weights of best model\n",
    "path = 'c1_fakenews_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq, test_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))\n",
    "\n",
    "## Fake News Predictions\n",
    "\n",
    "# load weights of best model\n",
    "path = 'c1_fakenews_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on unseen data\n",
    "unseen_news_text = [\"Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing\",     # Fake\n",
    "                    \"WATCH: George W. Bush Calls Out Trump For Supporting White Supremacy\",               # Fake\n",
    "                    \"U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources\",           # True\n",
    "                    \"Trump administration issues new rules on U.S. visa waivers\"                          # True\n",
    "                    ]\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "MAX_LENGHT = 15\n",
    "tokens_unseen = tokenizer.batch_encode_plus(\n",
    "    unseen_news_text,\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "unseen_seq = torch.tensor(tokens_unseen['input_ids'])\n",
    "unseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = model(unseen_seq, unseen_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
